{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4882d7af-e181-4e25-a5a8-7b9a7f78e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c4dac8-5fac-4f6a-9261-3c46d690979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data :  130\n",
      "Vocabulary size :  24\n",
      "Vocabulary :  {'b', 'h', '.', 't', 'm', ',', 'i', 'a', 'p', 'g', ' ', 'd', 's', 'r', 'n', 'y', 'k', 'u', 'e', 'v', 'o', 'w', 'l', 'T'}\n"
     ]
    }
   ],
   "source": [
    "file_data = open('data_cllm1.txt','r').read()\n",
    "data = list(file_data)\n",
    "vocab = set(file_data)\n",
    "data_size , vocab_size = len(data) , len(vocab)\n",
    "print('length of data : ',data_size)\n",
    "print('Vocabulary size : ',vocab_size)\n",
    "print('Vocabulary : ',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99d256c9-ed86-4293-a2b8-562902fe5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_chr = {ix : ch for ix,ch in enumerate(vocab)}\n",
    "chr_to_ix = {ch : ix for ix,ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37a73977-4544-4b93-a9c7-5f1efccd8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the rnn model with 100 neurons in the hidden layer\n",
    "hidden_neurons = 30\n",
    "sequence_length = 5\n",
    "# hyperparameters : Normalization\n",
    "wxh = np.random.randn(hidden_neurons,vocab_size) * 0.01\n",
    "whh = np.random.randn(hidden_neurons,hidden_neurons) * 0.01\n",
    "why = np.random.randn(vocab_size,hidden_neurons) * 0.01\n",
    "bh = np.zeros((hidden_neurons,1))\n",
    "by = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab73fc5-f99b-4bbd-99d4-1c7991c7ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_and_loss(inputs,targets,h_previous):\n",
    "    xs = {}\n",
    "    hs = {}\n",
    "    ys = {}\n",
    "    ps = {}\n",
    "    hs[-1] = h_previous  # initial hidden state before time stamp = 1\n",
    "    # feeding the inputs\n",
    "    loss = 0\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(wxh,xs[t]) + np.dot(whh,hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(why,hs[t]) + by\n",
    "        # probability distributions of the output :  softmax\n",
    "        ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        # summing up losses at each time stamp\n",
    "        # cross entropy loss\n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "\n",
    "        # computing loss\n",
    "    dwxh = np.zeros_like(wxh)\n",
    "    dwhh = np.zeros_like(whh)\n",
    "    dwhy = np.zeros_like(why)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dby = np.zeros_like(by)\n",
    "    #losses of the future time stamps as the loss wrt hidden state also influence the future hidden states \n",
    "    dhnext = np.zeros_like(bh) \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # calculating loss wrt output\n",
    "        # dy = dL/dy\n",
    "        dy = np.copy(ps[t])  # same as the probabilities if not target class\n",
    "        dy[targets[t]] -= 1 # Loss = probability - 1 if target class  \n",
    "        dwhy += np.dot(dy,hs[t].T) # dL/dwhy = dL/dy * dy/dwhy\n",
    "        dby += dy                 # dL/dby = dL/dy * dy/dby\n",
    "        # calculating loss wrt hidden state\n",
    "        # dh(t) = dL/dy * dy/dh  + gradient wrt the future time stamps \n",
    "        dh = np.dot(why.T,dy) + dhnext\n",
    "        # backpropagating in the tanh non-linearity in hidden states\n",
    "        # dh(z)/dp = (1 - h(z)^2) * dz/dp  where z is the input to the hidden state\n",
    "        dhraw = (1 - hs[t]**2) * dh\n",
    "        dbh += dhraw\n",
    "        dwhh += np.dot(dhraw,hs[t-1].T)\n",
    "        dwxh += np.dot(dhraw,xs[t].T)\n",
    "        # updating the gradients wrt the future hidden states\n",
    "        # dL/dh(t+1)\n",
    "        dhnext += np.dot(whh.T,dhraw)\n",
    "    # clipping to prevent exploding gradients\n",
    "    for dpara in [dwxh,dwhh,dwhy,dbh,dby]:\n",
    "        np.clip(dpara,-5,5,out=dpara)\n",
    "    return loss,dwxh,dwhh,dwhy,dbh,dby,hs[len(inputs)-1]  # last hidden states the sequence input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa7ef947-6572-4bb9-b80f-48be7787d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(hidden_state,seed,n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[seed] = 1\n",
    "    out = [ix_to_chr[seed]]\n",
    "    # generating output : \n",
    "    for t in range(n):\n",
    "        hidden_state = np.tanh(np.dot(wxh,x) + np.dot(whh,hidden_state) + bh)\n",
    "        y_out = np.dot(why,hidden_state) + by\n",
    "        y_out_p = np.exp(y_out)/np.sum(np.exp(y_out))\n",
    "        # find selection of the next character : \n",
    "        ix = np.random.choice(range(vocab_size),p = y_out_p.ravel())  # ravel is used for flatting\n",
    "        x = np.zeros_like(x)\n",
    "        x[ix] = 1\n",
    "        out.append(ix_to_chr[ix])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f908a580-8b83-4dda-a5c6-92f2a308397e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " TTvwt.ae,ws,tm,g,yguaiovthuyyhayrhsee.aohte,taaaigh ---\n",
      "\n",
      "--iter--0\t\t--loss--15.890269026945385\n",
      "---\n",
      " Twin le like de riamove the wi whar ih tThid w woa  ---\n",
      "\n",
      "--iter--1000\t\t--loss--9.47450519527242\n",
      "---\n",
      " Twinkle statdeikw the starere  whake, limoee kw ae  ---\n",
      "\n",
      "--iter--2000\t\t--loss--4.043688710504603\n",
      "---\n",
      " Twinkle little star, how ihwtnd w wwake ar, how in  ---\n",
      "\n",
      "--iter--3000\t\t--loss--1.6975580774825012\n",
      "---\n",
      " Tamond arld so voth, like a diamond in the swprp ar ---\n",
      "\n",
      "--iter--4000\t\t--loss--0.7628061938531507\n",
      "---\n",
      " Twinkle twikwl mowhi  i he s whyw nkle little star, ---\n",
      "\n",
      "--iter--5000\t\t--loss--0.6248169399951619\n",
      "---\n",
      " Twinkle little star, how i what you are, up above t ---\n",
      "\n",
      "--iter--6000\t\t--loss--0.35524066449877695\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--7000\t\t--loss--0.2525159016044389\n",
      "---\n",
      " Twinkle twinkle like, up atar, hohh s winkle twinkl ---\n",
      "\n",
      "--iter--8000\t\t--loss--0.2111693600398737\n",
      "---\n",
      " Twinkle twinkle twinde,rlate, uperohw i wonder ntat ---\n",
      "\n",
      "--iter--9000\t\t--loss--0.2852723900800338\n",
      "---\n",
      " Tup above ttwhiw ywinkle  winderr .p ab, hinbovh li ---\n",
      "\n",
      "--iter--10000\t\t--loss--0.20120329674061382\n",
      "---\n",
      " Twinklettwinkle little liardir wonder whatde, uorlb ---\n",
      "\n",
      "--iter--11000\t\t--loss--0.30212798276816955\n",
      "---\n",
      " Twinkle twinkle little star, how ske windyie ardhi  ---\n",
      "\n",
      "--iter--12000\t\t--loss--0.17855421858089313\n",
      "---\n",
      " Twinkle le ttwor ohe sky........................... ---\n",
      "\n",
      "--iter--13000\t\t--loss--0.11129384089379989\n",
      "---\n",
      " Twin tha di h, lin the sky......................... ---\n",
      "\n",
      "--iter--14000\t\t--loss--0.07673550580287407\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--15000\t\t--loss--0.05858447757684892\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--16000\t\t--loss--0.048294855147768295\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--17000\t\t--loss--0.04174288848625682\n",
      "---\n",
      " Twinkle twip ardoomonddhve tup above the world so h ---\n",
      "\n",
      "--iter--18000\t\t--loss--0.036937440502818275\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--19000\t\t--loss--0.033118338513527586\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--20000\t\t--loss--0.03008853069615482\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--21000\t\t--loss--0.027706540509828737\n",
      "---\n",
      " Twinkle twinkle likle twinkle little star, how i wo ---\n",
      "\n",
      "--iter--22000\t\t--loss--0.02580675520851243\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--23000\t\t--loss--0.024294290967729604\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--24000\t\t--loss--0.02312378481496127\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--25000\t\t--loss--0.022256525455413304\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--26000\t\t--loss--0.02165117235457148\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--27000\t\t--loss--0.021241839373151994\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--28000\t\t--loss--0.020955703239461176\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--29000\t\t--loss--0.02079075293415389\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--30000\t\t--loss--0.020709521689640617\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--31000\t\t--loss--0.02050376007930961\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--32000\t\t--loss--0.02026686034072611\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--33000\t\t--loss--0.020077890825327075\n",
      "---\n",
      " Twinkle twinkle little star, how i woner s iu the s ---\n",
      "\n",
      "--iter--34000\t\t--loss--0.019786154238502513\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--35000\t\t--loss--0.019458274369289597\n",
      "---\n",
      " Twinktwin leake.................................... ---\n",
      "\n",
      "--iter--36000\t\t--loss--0.36756784723985847\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--37000\t\t--loss--0.17333186773292508\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--38000\t\t--loss--0.08784619971079612\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--39000\t\t--loss--0.05256357262472345\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--40000\t\t--loss--0.03875357353948035\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--41000\t\t--loss--0.033966687782720426\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--42000\t\t--loss--0.031063729246566904\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--43000\t\t--loss--0.027675123400459162\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--44000\t\t--loss--0.024485227979971512\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--45000\t\t--loss--0.02207959380429164\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--46000\t\t--loss--0.020300967529459583\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--47000\t\t--loss--0.01894581930520454\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--48000\t\t--loss--0.01784182704700472\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--49000\t\t--loss--0.016886094825209953\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--50000\t\t--loss--0.01603500997477632\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--51000\t\t--loss--0.01526853144597\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--52000\t\t--loss--0.014574041695501956\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--53000\t\t--loss--0.013942421789942042\n",
      "---\n",
      " Twinkle twinkle little star, how i wonhe l star, sk ---\n",
      "\n",
      "--iter--54000\t\t--loss--0.013366652122567322\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--55000\t\t--loss--0.01284089835439438\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--56000\t\t--loss--0.012359957901399545\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--57000\t\t--loss--0.01191898910588383\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--58000\t\t--loss--0.01151341693805435\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--59000\t\t--loss--0.011138932637378328\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--60000\t\t--loss--0.010791537703039764\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--61000\t\t--loss--0.010467608619170109\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--62000\t\t--loss--0.010163968252855865\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--63000\t\t--loss--0.009877947125501805\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--64000\t\t--loss--0.00960741727951331\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--65000\t\t--loss--0.009350788913465193\n",
      "---\n",
      " Twinkle twinkle little star, how i wonder what you  ---\n",
      "\n",
      "--iter--66000\t\t--loss--0.009106968222453023\n"
     ]
    }
   ],
   "source": [
    "# epochs = 1000\n",
    "# epoch = 0\n",
    "p = 0  # data pointer initially set to 0 since we'll move from left to right in the data for training\n",
    "iteration = 0 # iteration counter\n",
    "# defining memory of gradients for each hyperparametres as used in adagrad optimization : \n",
    "mwxh = np.zeros_like(wxh)\n",
    "mwhh = np.zeros_like(whh)\n",
    "mwhy = np.zeros_like(why)\n",
    "mbh = np.zeros_like(bh)\n",
    "mby = np.zeros_like(by)\n",
    "smooth_loss = -np.log(1/vocab_size)*sequence_length\n",
    "\n",
    "while True and smooth_loss >= 0.009:\n",
    "    # check if we reached the end of data\n",
    "    if p+sequence_length >= data_size or iteration == 0: \n",
    "        # Reset the RNN model i.e, it's hidden states\n",
    "        p = 0\n",
    "        h_previous = np.zeros((hidden_neurons,1))\n",
    "        # epoch += 1\n",
    "    # Prepare the inputs and targets\n",
    "    inputs = [chr_to_ix[ch] for ch in data[p:p+sequence_length]]\n",
    "    targets = [chr_to_ix[ch] for ch in data[p+1:p+sequence_length+1]]\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        # sample : \n",
    "        sample_ix = sample(h_previous,inputs[0],50)\n",
    "        txt = ''.join(sample_ix)\n",
    "        print('---\\n',txt,'---\\n')\n",
    "\n",
    "    # calculate the loss after feeding the inputs to the rnn:\n",
    "    loss,dwxh,dwhh,dwhy,dbh,dby,h_previous = feed_and_loss(inputs,targets,h_previous)\n",
    "    # exponentially moving average of the loss\n",
    "    smooth_loss = smooth_loss * 0.999 + loss*0.001\n",
    "    if iteration % 1000 == 0:\n",
    "        print(f'--iter--{iteration}\\t\\t--loss--{smooth_loss}')\n",
    "    # updating the hyper parametres using adagrad optimization : \n",
    "    for para , dpara, mem_para in zip([wxh,whh,why,bh,by],[dwxh,dwhh,dwhy,dbh,dby],[mwxh,mwhh,mwhy,mbh,mby]) : \n",
    "        # updation  in memory : \n",
    "        mem_para += dpara**2\n",
    "        para -= 0.1 * dpara / np.sqrt(mem_para + 1e-8) # adagrad\n",
    "    p += sequence_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aab53774-f066-46ab-a2a1-ea7665b0ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twinkle twinkle little star, how i wonder what you are, up above the world so high, like a diamond in the sky...\n"
     ]
    }
   ],
   "source": [
    "sample_ix = sample(np.zeros((hidden_neurons,1)),chr_to_ix['T'],111)\n",
    "txt = ''.join(sample_ix)\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
